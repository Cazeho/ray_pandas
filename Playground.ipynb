{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import time \n",
    "import numpy as np\n",
    "import binascii\n",
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IDENTIFIER_LENGTH = 20\n",
    "\n",
    "# This prefix must match the value defined in ray_redis_module.cc.\n",
    "DB_CLIENT_PREFIX = b\"CL:\"\n",
    "\n",
    "\n",
    "def hex_identifier(identifier):\n",
    "    return binascii.hexlify(identifier).decode()\n",
    "\n",
    "\n",
    "def identifier(hex_identifier):\n",
    "    return binascii.unhexlify(hex_identifier)\n",
    "\n",
    "def key_to_hex_identifiers(key):\n",
    "    # Extract worker_id and task_id from key of the form\n",
    "    # prefix:worker_id:task_id.\n",
    "    offset = key.index(b\":\") + 1\n",
    "    worker_id = hex_identifier(key[offset:(offset + IDENTIFIER_LENGTH)])\n",
    "    offset += IDENTIFIER_LENGTH + 1\n",
    "    task_id = hex_identifier(key[offset:(offset + IDENTIFIER_LENGTH)])\n",
    "    return worker_id, task_id\n",
    "\n",
    "import re\n",
    "def clean(sometext):\n",
    "    sometext = sometext.decode('UTF-8')\n",
    "    ansi_escape = re.compile(r'\\x1b[^m]*m')\n",
    "    return ansi_escape.sub('', sometext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(num_cpus=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function definitions...\n",
    "\n",
    "@ray.remote\n",
    "def example(x):\n",
    "    time.sleep(np.random.random())\n",
    "    return np.random.randn()\n",
    "\n",
    "@ray.remote\n",
    "class TestCls():\n",
    "    def __init__(self):\n",
    "        self.g = 1\n",
    "        \n",
    "    def to_go(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "@ray.remote\n",
    "class Outer():\n",
    "    def __init__(self):\n",
    "        self.f = 1\n",
    "        self.test = TestCls.remote()\n",
    "    \n",
    "    def to_go2(self, x):\n",
    "        return x * 2\n",
    "    \n",
    "    def error(self):\n",
    "        return 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we generate data in redis for remote tasks\n",
    "results = ray.get([example.remote(x) for x in range(4)])\n",
    "\n",
    "# Generating data for Actor tasks\n",
    "actor = TestCls.remote()\n",
    "actor_results = ray.get([actor.to_go.remote(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_state = ray.worker.global_state # Used to access redis client, but also has nice interface for certain information \n",
    "gworker = ray.worker.global_worker # Unused for now\n",
    "rc_non = global_state.redis_client # Redis Client for interacting with redis without decoding\n",
    "addr, port = gworker.redis_address.split(\":\")\n",
    "rc = redis.StrictRedis(host=addr, port=port, decode_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key things included in redis from `rc.keys()`:\n",
    " - Event log\n",
    " - worker info\n",
    " - Remote functions\n",
    " - \"Functions to run\"\n",
    " - Drivers\n",
    " - Redis clients\n",
    " - Actor classes\n",
    " - Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter() # for printing dicts and lists in a manner easy for the eyes\n",
    "\n",
    "\n",
    "for k in rc.keys():\n",
    "    try:\n",
    "        print(k)\n",
    "        pp.pprint(rc.hgetall(k))\n",
    "#         rc.hgetall(k) \n",
    "    except Exception:\n",
    "        print(k, \"Failed \")\n",
    "# Certain return values, such as `event_log:*`, can only be accessed via list calls to redis\n",
    "\n",
    "#         try:\n",
    "#             print('#' * 10)\n",
    "#             print(k)\n",
    "#             pp.pprint(rc.lrange(k, 0, -1))\n",
    "#         except Exception:\n",
    "#             print(k, \"Failed \")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import unicode \n",
    "actor_list = []\n",
    "for a_key in rc.keys(\"Actor:*\"):\n",
    "    v = rc.hgetall(a_key)\n",
    "    v['actor_id'] = a_key\n",
    "    v['class_id'] = hex_identifier(v['class_id'])\n",
    "    actor_list.append(v)\n",
    "\n",
    "actor_df = pd.DataFrame(actor_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Fix actor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor_classes = []\n",
    "for a_key in rc.keys(\"ActorClass:*\"):\n",
    "    \n",
    "    v = rc.hgetall(a_key)\n",
    "    del v['class'] # removed pickle hex for readability\n",
    "    v['driver_id'] = hex_identifier(v['driver_id'])\n",
    "    class_id = a_key.split(b':')[1]\n",
    "    v['class_id'] = hex_identifier(class_id)\n",
    "    actor_classes.append(v)\n",
    "    \n",
    "actor_class_df = pd.DataFrame(actor_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_state.client_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_state.function_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dict = {oid.hex(): v for oid, v in global_state.object_table().items()}\n",
    "object_df = pd.DataFrame(object_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = global_state.task_table()\n",
    "tt_list = list(tt.values())\n",
    "\n",
    "for d in tt_list:\n",
    "    d['TaskSpec']['ReturnObjectIDs'] = [oid.hex() for oid in d['TaskSpec']['ReturnObjectIDs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "result = json_normalize(tt_list)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also parse the event logs in order to get fine grained timing for remote tasks. However, as a user, I'd probably only care about time taken in running the task -- this can be much refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "event_list = []\n",
    "\n",
    "# Get and decode all task timing/event logs\n",
    "for key in rc.keys(\"event_log*\"):\n",
    "    content = rc.lrange(key, 0, -1)\n",
    "#     event_list.append(json.loads(content[0].decode())) \n",
    "    event_list.append(json.loads(content[0]))\n",
    "    \n",
    "# event_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This seems to be the best way to do the event_log -> dataframe pipeline. \n",
    "# First generate a (key, [value]) mapping for all tasks and then apply some pandas operations to convert.\n",
    "\n",
    "# event_dict is used to store timing info\n",
    "event_dict = defaultdict(lambda: np.full(len(event_list), np.nan))\n",
    "\n",
    "# info_dict is used to store meta data - such as function names and task id\n",
    "info_dict = defaultdict(lambda: [None] * len(event_list))\n",
    "\n",
    "for i, task_event in enumerate(event_list):\n",
    "    for event in (task_event):\n",
    "        time, label, startstop, info = event\n",
    "        event_dict[(label, startstop)][i] = time\n",
    "        if info:\n",
    "            for k in info:\n",
    "                info_dict[k][i] = info[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The tuple keys for `event_dict` produce a hierarchical index, which could be useful. However, joining or merging it \n",
    "# with other non-hierarchical indices will throw away this structure.\n",
    "\n",
    "edf = pd.DataFrame(event_dict) \n",
    "edf.rename(columns={1: 'start', 2:'end'}, inplace=True)\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.DataFrame(info_dict)\n",
    "# idf.columns = pd.MultiIndex.from_tuples([(c, '') for c in idf]) # this is non-idempotent!\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote_table = pd.concat([idf, edf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we join two tables together as an example\n",
    "idf.merge(result, left_on=\"task_id\", right_on=\"TaskSpec.TaskID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example to get Error, function_name, IP address, Actor ID, ParentID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Error messages logging (specifics - which node, which function call, which actor, what time)\n",
    "  - Getting Error messages from Redis is redundant because we already get info in the event_log. Error messages provide extra information such as `error_id` and `type`, which don't seem particularly useful.\n",
    "  - Create table for workers (Id, socket info, node IP address)\n",
    "  - Get multinode setting - test out client table\n",
    "  - Write out an example for error tracing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
