{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import time \n",
    "import numpy as np\n",
    "import binascii\n",
    "import redis\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter() # for printing dicts and lists in a manner easy for the eyes\n",
    "from misc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for redis server at 127.0.0.1:47883 to respond...\n",
      "Waiting for redis server at 127.0.0.1:15885 to respond...\n",
      "Starting local scheduler with 3 CPUs and 0 GPUs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'local_scheduler_socket_names': ['/tmp/scheduler42909302'],\n",
       " 'node_ip_address': '127.0.0.1',\n",
       " 'object_store_addresses': [ObjectStoreAddress(name='/tmp/plasma_store63297133', manager_name='/tmp/plasma_manager45575887', manager_port=53037)],\n",
       " 'redis_address': '127.0.0.1:47883'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(num_cpus=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below, we define some example functions for populating the redis database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function definitions...\n",
    "\n",
    "@ray.remote\n",
    "def example(x):\n",
    "    time.sleep(np.random.random())\n",
    "    return np.random.randn()\n",
    "\n",
    "@ray.remote\n",
    "class TestCls():\n",
    "    def __init__(self):\n",
    "        self.g = 1\n",
    "        \n",
    "    def to_go(self, x):\n",
    "        return x\n",
    "\n",
    "    \n",
    "@ray.remote\n",
    "class Outer():\n",
    "    def __init__(self):\n",
    "        self.f = 1\n",
    "        self.test = TestCls.remote()\n",
    "    \n",
    "    def to_go2(self, x):\n",
    "        return x * 2\n",
    "    \n",
    "    def error(self):\n",
    "        return 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we generate data in redis for remote tasks\n",
    "results = ray.get([example.remote(x) for x in range(4)])\n",
    "\n",
    "# Generating data for Actor tasks\n",
    "actor = TestCls.remote()\n",
    "actor_results = ray.get([actor.to_go.remote(1)])\n",
    "\n",
    "err_actor = Outer.remote()\n",
    "err_actor.error.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_state = ray.global_state # Used to access redis client, but also has nice interface for certain information \n",
    "addr, port = ray.worker.global_worker.redis_address.split(\":\")\n",
    "rc = redis.StrictRedis(host=addr, port=port, decode_responses=True)\n",
    "# rc_non = global_state.redis_client # Redis Client for interacting with redis without decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From checking the values of the redis store, we can identify some key things included via `rc.keys()`:\n",
    " - Event log\n",
    " - worker info\n",
    " - Remote functions\n",
    " - \"Functions to run\"\n",
    " - Drivers\n",
    " - Redis clients\n",
    " - Actor classes\n",
    " - Actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k in rc.keys():\n",
    "#     try:\n",
    "#         print(k)\n",
    "#         pp.pprint(rc.hgetall(k))\n",
    "# #         rc.hgetall(k) \n",
    "#     except Exception:\n",
    "#         print(k, \"Failed \")\n",
    "# # Certain return values, such as `event_log:*`, can only be accessed via list calls to redis\n",
    "\n",
    "#         try:\n",
    "#             print('#' * 10)\n",
    "#             print(k)\n",
    "#             pp.pprint(rc.lrange(k, 0, -1))\n",
    "#         except Exception:\n",
    "#             print(k, \"Failed \")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO: Fix actor_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can populate a DataFrame with Actor \n",
    "actor_list = []\n",
    "for a_key in rc.keys(\"Actor:*\"):\n",
    "    v = rc.hgetall(a_key)\n",
    "    v['actor_id'] = a_key\n",
    "    v['class_id'] = hex_identifier(v['class_id'])\n",
    "    actor_list.append(v)\n",
    "\n",
    "actor_df = pd.DataFrame(actor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Classes Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor_classes = []\n",
    "for a_key in rc.keys(\"ActorClass:*\"):\n",
    "    \n",
    "    v = rc.hgetall(a_key)\n",
    "    del v['class'] # removed pickle hex for readability\n",
    "    v['driver_id'] = hex_identifier(v['driver_id'])\n",
    "    class_id = a_key.split(b':')[1]\n",
    "    v['class_id'] = hex_identifier(class_id)\n",
    "    actor_classes.append(v)\n",
    "    \n",
    "actor_class_df = pd.DataFrame(actor_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_class_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote Functions Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_table = global_state.function_table()\n",
    "fn_list = []\n",
    "for fn_id in fn_table:\n",
    "    val = fn_table[fn_id]\n",
    "    val[\"function_id\"] = fn_id\n",
    "    fn_list.append(val)\n",
    "pd.DataFrame(fn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuxAddress</th>\n",
       "      <th>ClientType</th>\n",
       "      <th>DBClientID</th>\n",
       "      <th>Deleted</th>\n",
       "      <th>LocalSchedulerSocketName</th>\n",
       "      <th>NumCPUs</th>\n",
       "      <th>NumGPUs</th>\n",
       "      <th>node_ip_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>plasma_manager</td>\n",
       "      <td>0d0de2a4834950c2300fc56c30a35f3aaebf82e1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127.0.0.1:53037</td>\n",
       "      <td>local_scheduler</td>\n",
       "      <td>29b3e3c39c6318351fda75dac0c9d2405df190e0</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmp/scheduler42909302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127.0.0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>global_scheduler</td>\n",
       "      <td>7ecef464394486465ec9a0e0c4e82fbae36b9c1d</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.0.0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AuxAddress        ClientType  \\\n",
       "0              NaN    plasma_manager   \n",
       "1  127.0.0.1:53037   local_scheduler   \n",
       "2              NaN  global_scheduler   \n",
       "\n",
       "                                 DBClientID Deleted LocalSchedulerSocketName  \\\n",
       "0  0d0de2a4834950c2300fc56c30a35f3aaebf82e1   False                      NaN   \n",
       "1  29b3e3c39c6318351fda75dac0c9d2405df190e0   False   /tmp/scheduler42909302   \n",
       "2  7ecef464394486465ec9a0e0c4e82fbae36b9c1d   False                      NaN   \n",
       "\n",
       "   NumCPUs  NumGPUs node_ip_address  \n",
       "0      NaN      NaN       127.0.0.1  \n",
       "1      3.0      0.0       127.0.0.1  \n",
       "2      NaN      NaN       127.0.0.1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the global state API, we can populate a DataFrame with a list of Redis Clients currently connected\n",
    "ctable = global_state.client_table()\n",
    "\n",
    "client_list = []\n",
    "for node_ip in ctable:\n",
    "    for client in ctable[node_ip]:\n",
    "        client[\"node_ip_address\"] = node_ip\n",
    "        client_list.append(client)\n",
    "\n",
    "client_df = pd.DataFrame(client_list)\n",
    "client_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can populate a DataFrame with a list of objects in the object store\n",
    "object_dict = {oid.hex(): v for oid, v in global_state.object_table().items()}\n",
    "object_df = pd.DataFrame(object_dict).transpose()\n",
    "object_df\n",
    "# May need to change this so ID is not index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "tt = global_state.task_table()\n",
    "tt_list = list(tt.values())\n",
    "\n",
    "for d in tt_list:\n",
    "    d['TaskSpec']['ReturnObjectIDs'] = [oid.hex() for oid in d['TaskSpec']['ReturnObjectIDs']]\n",
    "\n",
    "task_df = json_normalize(tt_list)\n",
    "\n",
    "task_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also parse the event logs in order to get fine grained timing for remote tasks. However, as a user, I'd probably only care about time taken in running the task -- this can be much refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json \n",
    "event_list = []\n",
    "\n",
    "# Get and decode all task timing/event logs\n",
    "for key in rc.keys(\"event_log*\"):\n",
    "    content = rc.lrange(key, 0, -1)\n",
    "#     event_list.append(json.loads(content[0].decode())) \n",
    "    event_list.append(json.loads(content[0]))\n",
    "    \n",
    "# event_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This seems to be the best way to do the event_log -> dataframe pipeline. \n",
    "# First generate a (key, [value]) mapping for all tasks and then apply some pandas operations to convert.\n",
    "\n",
    "# event_dict is used to store timing info\n",
    "event_dict = defaultdict(lambda: np.full(len(event_list), np.nan))\n",
    "\n",
    "# info_dict is used to store meta data - such as function names and task id\n",
    "info_dict = defaultdict(lambda: [None] * len(event_list))\n",
    "\n",
    "for i, task_event in enumerate(event_list):\n",
    "    for event in (task_event):\n",
    "        time, label, startstop, info = event\n",
    "        event_dict[(label, startstop)][i] = time\n",
    "        if info:\n",
    "            for k in info:\n",
    "                info_dict[k][i] = info[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The tuple keys for `event_dict` produce a hierarchical index, which could be useful. However, joining or merging it \n",
    "# with other non-hierarchical indices will throw away this structure.\n",
    "\n",
    "edf = pd.DataFrame(event_dict) \n",
    "edf.rename(columns={1: 'start', 2:'end'}, inplace=True)\n",
    "edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.DataFrame(info_dict)\n",
    "# idf.columns = pd.MultiIndex.from_tuples([(c, '') for c in idf]) # this is non-idempotent!\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remote_table = pd.concat([idf, edf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to get Error, function_name, IP address, Actor ID, ParentID\n",
    "\n",
    "errors = idf[idf.traceback.notnull()]\n",
    "error_task = errors.merge(task_df, left_on=\"task_id\", right_on=\"TaskSpec.TaskID\")\n",
    "err_task_loc = error_task.merge(client_df, left_on=\"LocalSchedulerID\", right_on=\"DBClientID\")\n",
    "err_task_loc[['function_name', 'traceback', 'value', 'TaskSpec.ActorID',\n",
    "              'TaskSpec.ParentTaskID',  'node_ip_address',  'task_id',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Error messages logging (specifics - which node, which function call, which actor, what time)\n",
    "  - Getting Error messages from Redis is redundant because we already get info in the event_log. Error messages provide extra information such as `error_id` and `type`, which don't seem particularly useful.\n",
    "  - ~Create table for workers (Id, socket info, node IP address) ... are workers even still a proper abstraction~\n",
    "  - Get multinode setting - test out client table\n",
    "  - !! Write out an example for error tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
